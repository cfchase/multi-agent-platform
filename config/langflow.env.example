# LangFlow Container Environment Variables
# Copy to langflow.env and fill in values.
# These are forwarded to the LangFlow container as global variables
# so flows can reference them without hardcoding secrets.

# LLM API Keys (optional â€” only set the providers you use)
OPENAI_API_KEY=
GEMINI_API_KEY=
ANTHROPIC_API_KEY=

# Ollama base URL (for local LLM inference)
# Default: http://host.containers.internal:11434 (container-to-host networking)
# OLLAMA_BASE_URL=http://host.containers.internal:11434
